# ğŸ‹ï¸ Hybrid Reinforcement Learning for CartPole

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch\&logoColor=white)](https://pytorch.org/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-CartPole-green)](https://gymnasium.farama.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**Comparing Hill Climbing, Double Deep Q-Networks (DDQN), and a novel Hybrid approach for solving the classic CartPole problem.**

ğŸ¥ [Demo Video (Coming Soon)](#)

</div>  

---

## ğŸŒŸ Overview

This project explores **different reinforcement learning strategies** to master the `CartPole-v1` environment.
The unique contribution is a **Hybrid RL technique**: bootstrapping a DDQN with experience from a simple Hill Climbing agent, accelerating learning and improving stability.

### âœ¨ Highlights

* ğŸ¯ **Algorithmic Comparison**: Hill Climbing, DDQN, Hybrid (Hill Climbing + DDQN).
* ğŸš€ **Hybrid Warm-Start**: Replay buffer pre-filled with Hill Climbing trajectories.
* ğŸ“Š **Analytics & Visualization**: Matplotlib plots comparing rewards & accuracy.
* ğŸ–¥ï¸ **Live Rendering**: Watch the best trained agent perform via Pygame.

---

## ğŸ“‚ Project Structure

```plaintext
CartPole-Hybrid-RL/
â”œâ”€â”€ code.py                         # Main script (training + evaluation + visualization)
â”œâ”€â”€ Project_Overview_and_Literature_Review.pdf
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### Prerequisites

* Python 3.9+
* pip package manager
* Git

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/YOUR_USERNAME/CartPole-Hybrid-RL.git
cd CartPole-Hybrid-RL

# 2ï¸âƒ£ (Optional) Setup virtual environment
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
```

**requirements.txt**

```
numpy
gym[classic_control]
matplotlib
pygame
torch
```

---

## ğŸš€ Usage

Run the full experiment (training + plotting + visualization):

```bash
python code.py
```

### Execution Flow

1. ğŸ§— Train **Hill Climbing agent**.
2. ğŸ§  Train **DDQN agent** from scratch.
3. ğŸ’¡ Train **Hybrid DDQN** (warm-started with Hill Climbing data).
4. ğŸ“ˆ Plot **rewards & accuracy** comparisons.
5. ğŸ¥ Render **live agent performance** (Pygame).

---

## ğŸ“Š Results

<div align="center">

### Reward & Accuracy Comparison

Hybrid (blue) often converges faster and outperforms DDQN (orange).

![Comparison 1](https://github.com/user-attachments/assets/e34c7303-5ea4-47ec-8fd2-c02a4f17ddb3)
![Comparison 2](https://github.com/user-attachments/assets/07601e5b-3eb3-4ce0-ad87-070e06681832)

### Live Agent Demo

*A GIF/video here would boost the README a lot.*

</div>  

---

## ğŸ§  Technical Details

### Algorithms

* **Hill Climbing** â†’ Linear policy, optimized with noisy perturbations.
* **DDQN** â†’ PyTorch model with two hidden layers (64 neurons, ReLU).
* **Hybrid DDQN** â†’ Replay buffer pre-filled by Hill Climbing rollouts.

### Training Setup

* Replay Memory: `deque(maxlen=10,000)`
* Optimizer: Adam
* Loss: MSE
* Exploration: Epsilon-greedy (`1.0 â†’ 0.01`)
* Hybrid Pre-fill: \~50 episodes from Hill Climbing agent

---

## ğŸ”® Future Work

* [ ] Add PPO / A2C baselines
* [ ] Hyperparameter tuning (Optuna/Ray Tune)
* [ ] Extend to harder environments (`LunarLander-v2`, `MountainCar-v0`)
* [ ] Save/load trained models
* [ ] Web UI demo (Streamlit/Gradio)

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create your branch (`feature/AmazingFeature`)
3. Commit changes (`git commit -m "Add AmazingFeature"`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open PR ğŸš€

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file.

---

## ğŸ™ Acknowledgments

* **OpenAI Gymnasium** for CartPole environment
* **PyTorch** for deep learning framework

---

## ğŸ‘¨â€ğŸ’» Author

<div align="center">

**[A. Jayavanth](https://github.com/jayavanth18)**

[![GitHub](https://img.shields.io/badge/GitHub-jayavanth18-black?logo=github)](https://github.com/jayavanth18)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?logo=linkedin)](https://www.linkedin.com/in/jayavanth18/)

â­ If this repo helped you, consider giving it a **star**!

</div>  

---
